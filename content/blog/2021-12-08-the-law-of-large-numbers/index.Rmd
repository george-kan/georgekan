---
title: The Law of Large Numbers
date: '2021-12-08'
slug: []
categories: []
tags:
  - statistics
subtitle: ''
excerpt: ''
draft: yes
series: ~
layout: single
---


```{r include=F}
library(emo)
library(ggplot2)
library(gganimate)
library(dplyr)
library(scales)


theme_set(theme_minimal(base_size=25) +
            theme(plot.title = element_text(size = 20),
                  axis.text = element_text(size=16),
                  legend.text = element_text(size = 14)))
```

### Introduction

I decided after years of struggling with remembering statistical concepts that I have to try to explain them so that:  
1. I am forced to understand them better `r emo::ji("smile")`  
2. Anybody can read and comment useful ideas that I might have missed  
3. I will be able to come back to my explanation once I forget the concept

Please note that this is not a very math heavy collection of posts, my goal is to get a better intuitive understanding of the concepts.  
With all that being said, this is the first post in my statistics series and it is about the **Law of Large Numbers!**


### Definition

**The average of the results obtained from a large number of trials should be close to the expected value and will tend to become closer to the expected value as more trials are performed**

Let's try to map the words in the definition to mathematical constructions.

<span style="color:purple"> Large number of trials </span>

Here we assume that we are doing multiple trials coming from the same underlying probability distribution. 

We can model these trials as n random variables:
$$ X_1, X_2, ..., X_n $$

These variables are identically distributed (so all follow the same distribution) and they are independent (knowledge about the outcome of one of them does not provide any information about the others). More trials means higher `n`.


<span style="color:purple"> Average of the results </span>

The mathematical representation of the average of the results is the sample mean or sample average:
$$ \overline{X_n} = \frac {X_1 + X_2+...+X_n} n $$


<span style="color:purple"> Expected value </span>

The [expected value](https://en.wikipedia.org/wiki/Expected_value) of a random variable can be thought of intuitively as the arithmetic mean of a large number of independent realizations of X. (This is not the topic of this post so I will not go into more detail)  
The expectation of our random variables is the following:

$$ E[X_1] = E[X_2] = ... = E[X_n] = E[X] = \mu $$

Unfortunately, in many scenarios we do not know what value Î¼ takes therefore we need something to estimate it.  
This is where the Law of Large Numbers comes in and tells us that the sample mean should be close to the expected value and this "closeness" increases with the number of trials.  
Stated in a mathematical formulation:

$$ \overline{X_n} \xrightarrow[n\rarr \infty]{P} E[X] $$


### In action

We are going to see three examples of the Law of Large Numbers for the following three distributions:  
1. Standard normal (Continuous random variable)  
2. Binomial (Discrete random variable)     
3. My own distribution (I have not coined it yet but it's happening! `r emo::ji("smile")`) (Mixed random variable) 

**1. Trials from a standard normal distribution**

In our first example, we assume the following:

$$ X_1, X_2, ..., X_n \thicksim N(0,1)\ i.i.d.$$

Just as a reminder, this is what the standard normal looks like: 

```{r echo=F, warning=FALSE}
 ggplot() +
  geom_function(fun = dnorm, size=1, color = "blue") +
  geom_vline(aes(xintercept = 0), color="palegreen2", size=0.8) + 
  annotate("text", x = 1.3, y = 0.04, label="Expected\n value", size=6) +
  geom_curve(aes(x = 0.9, xend=0.01, y=0.02, yend=0.1), arrow = arrow(length = unit(0.07, "inch")), size = 1,
    color = "gray20", curvature = -0.3) +
  scale_x_continuous(limits = c(-4, 4), breaks = -4:4) +
  labs(title = "Standard normal distribution") +
  theme(axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5))
```

We perform 500 trials from this distribution. Let's see how the sample mean changes as the number of trials increases.
```{r include=F}
set.seed(1)
n=500
sn_realisations = rnorm(n)
sample_means = cummean(sn_realisations)

plot_dt = data.frame(samples = 1:n, Xbar = sample_means)

```


```{r echo=FALSE}

p1 = ggplot(plot_dt, aes(x = samples, y = Xbar)) +
            geom_line(color="firebrick") +
            geom_hline(yintercept = 0, color="palegreen", size=0.8) +
            annotate("text", x = 30, y = 0.4, label="Expected\n value", color=muted("green"), size=5) +
            geom_curve(aes(x = 30, xend=30, y=0.32, yend=0), arrow = arrow(length = unit(0.07, "inch")), size = 1,
            color = "gray20", curvature = -0.2) +
            labs(y = "Sample mean", title="Sample mean as trials increase", x="trials") + 
            scale_x_continuous(breaks = seq(50, n, by = 50)) +
            scale_y_continuous(limits = c(min(plot_dt$Xbar), 0.5)) +
            theme(axis.title.x = element_text(margin = margin(t = 30, r = 0, b = 0, l = 0)))

graph2.animation<-p1 + transition_reveal(samples) + view_follow(fixed_y = TRUE)
animate(graph2.animation, height = 800, width = 800, fps = 10, duration = 5,
        end_pause = 10, res = 100)


```

In the annimated chart above we can see how the sample average changes as trials increase. At the beginning it is far from the expected value (marked in green) but as the number of trials grows larger it gets closer and closer. 


**2. Unbiased coin flip**

The next example is a very typical one, it is the toss of an unbiased coin. 
In particular, we model the toss of an unbiased coin by using a Bernoulli distribution with the following mapping:  

* Heads: The outcome of the Bernoulli is 1  
* Tails: The outcome of the Bernoulli is 0  


```{r echo=FALSE, fig.width=8}
bernoulli_df = data.frame(x = c(0, 1), y = c(0.5, 0.5))

ggplot(bernoulli_df, aes(x=x)) +
  geom_point(aes(y=y), size = 6, color="blue") +
  geom_segment(aes(xend=x, y=0, yend=y), color="blue", size = 2) +
  geom_vline(aes(xintercept = 0.5), color="palegreen2", size=0.8) + 
  annotate("text", x = 0.75, y = 0.25, label="Expected value", size=6) +
  geom_curve(aes(x = 0.7, xend=0.5, y=0.2, yend=0.2), arrow = arrow(length = unit(0.07, "inch")), size = 1,
             color = "gray20", curvature = -0.1) + 
  scale_x_continuous(breaks = c(0, 0.5, 1), labels = c("Tails (0)", "0.5", "Heads (1)")) +
  scale_y_continuous(limits = c(0,1)) +
  labs(title = "Probability density of an unbiased coin") + 
  theme(panel.grid.minor = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5))

```

The expected value of an unbiased coin toss is 0.5 or equivalently phrased you expect to get half heads and half tails. 
Mathematically, we assume the following:

$$ X_1, X_2, ..., X_n \thicksim Ber(0.5)\ i.i.d.$$

We perform 500 trials from this distribution. Let's see how the sample mean changes as the number of trials increases.
```{r include=F}
set.seed(10)
n=500
bi_realisations = rbinom(n, 1, 0.5)
sample_means = cummean(bi_realisations)

plot_dt = data.frame(samples = 1:n, Xbar = sample_means)

```


```{r echo=FALSE}

p2 = ggplot(plot_dt, aes(x = samples, y = Xbar)) +
            geom_line(color="firebrick") +
            geom_hline(yintercept = 0.5, color="palegreen", size=0.8) +
            annotate("text", x = 50, y = 0.75, label="Expected\n value", color=muted("green"), size=5) +
            geom_curve(aes(x = 45, xend=30, y=0.66, yend=0.5), arrow = arrow(length = unit(0.07, "inch")), size = 1,
            color = "gray20", curvature = -0.2) +
            scale_x_continuous(breaks = seq(50, n, by = 50)) +
            scale_y_continuous(limits = c(0,1)) +
            labs(y = "Sample mean", title="Sample mean as trials increase", x="trials") + 
            theme(axis.title.x = element_text(margin = margin(t = 30, r = 0, b = 0, l = 0)))

graph2.animation <-p2 + transition_reveal(samples) + view_follow(fixed_y = TRUE)
animate(graph2.animation, height = 800, width = 800, fps = 10, duration = 5,
        end_pause = 10, res = 100)


```

Again we see that as the number of trials increases, the sample mean approaches the expected value.

**3. Random distribution I came up with**

As a last example, we are going to illustrate the Law of Large numbers for an arbitrary distribution that I came up with.  
Let's make this even more interesting and pick a mixed distribution. Just as a reminder, a mixed distribution is a distribution that is neither continuous nor discrete. In some parts, it is continuous and in some parts it is discrete.


```{r echo=F}

str_df = data.frame(x=1.5, y=0.75)

ggplot(data = str_df) +
  geom_segment(aes(x = 0, xend=1, y=0.25, yend=0.25), size = 1, color="blue") +
  geom_segment(aes(x = 0, xend=0, y=0, yend=0.25), size = 1, color="blue") +
  geom_segment(aes(x = 1, xend=1, y=0, yend=0.25), size = 1, color="blue") +
  geom_segment(aes(xend=x, x=x, y=0, yend=y), color = "blue", size=2) + 
  geom_point(aes(x = 1.5, y=0.75), size=6, color = "blue") + 
  geom_vline(xintercept = 1.25, color = "palegreen2", size=0.8) +  
  annotate("text", x = 0.75, y = 0.5, label="Expected\n value", size=6) +
  geom_curve(aes(x = 0.9, xend=1.25, y=0.5, yend=0.5), arrow = arrow(length = unit(0.07, "inch")), size = 1,
    color = "gray20", curvature = -0.3) +
  scale_y_continuous(breaks = seq(0, 0.75, 0.25)) + 
  scale_x_continuous(breaks = seq(0, 1.5, 0.25)) +
  labs(title = "The 'George' distribution") + 
  theme(panel.grid.minor = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5))




```

As we can see from the plot above, the 'George' distribution consists of the following two parts:  

1. Continuous: We have a uniform from 0 to 1 with a height of 0.25
2. Discrete: There is a 0.75 probability that a sample from the 'George' will take a value of 1.5

It's a good idea to check if the total probability sums to 1:

$$ Total\ probability = 0.25*1 (rectangle\ area) + 0.75 = 1$$

We perform again 500 trials from this distribution. Let's see how the sample mean changes as the number of trials increases.
```{r include=F}
set.seed(100)
n=500
uni_samples = runif(n)

george_realisations = ifelse(uni_samples >= 0.25, 1.5, uni_samples*4)
sample_means = cummean(george_realisations)

plot_dt = data.frame(samples = 1:n, Xbar = sample_means)

```


```{r echo=F}

p3 = ggplot(plot_dt, aes(x = samples, y = Xbar)) +
            geom_line(color="firebrick") +
            geom_hline(yintercept = 1.25, color="palegreen", size=0.8) +
            annotate("text", x = 50, y = 1.45, label="Expected\n value", color=muted("green"), size=5) +
            geom_curve(aes(x = 45, xend=30, y=1.42, yend=1.25), arrow = arrow(length = unit(0.07, "inch")), size = 1,
            color = "gray20", curvature = -0.1) +
            scale_x_continuous(breaks = seq(50, n, by = 50)) +
            scale_y_continuous(limits = c(1.15, 1.52)) +
            labs(y = "Sample mean", title="Sample mean as trials increase", x="trials") + 
            theme(axis.title.x = element_text(margin = margin(t = 30, r = 0, b = 0, l = 0)))

graph2.animation <-p3 + transition_reveal(samples) + view_follow(fixed_y = TRUE)
animate(graph2.animation, height = 800, width = 800, fps = 10, duration = 5,
        end_pause = 10, res = 100)

```

<span style="color:red">Ok clearly something went terribly wrong! Here I have been plotting throughout this blog showing you that the red line eventually gets really really close to the green one but clearly it is not the case!</span>  
Did I miscalculate the expectation of my own distribution or is the Law of Large Numbers a hoax? 

Surprisingly neither! 

The Law of Large Numbers does not state anything about the speed of convergence. It only states that as the number of trials gets larger, the sample mean will get closer and closer to the expectated value.  
Let's try it again but with a higher number of trials this time: 


```{r include=F}
set.seed(100)
n=10000
uni_samples = runif(n)

george_realisations = ifelse(uni_samples >= 0.25, 1.5, uni_samples*4)
sample_means = cummean(george_realisations)

plot_dt = data.frame(samples = 1:n, Xbar = sample_means)

```



```{r echo=F}

p4 = ggplot(plot_dt, aes(x = samples, y = Xbar)) +
            geom_line(color="firebrick") +
            geom_hline(yintercept = 1.25, color="palegreen", size=0.8) +
            annotate("text", x = 1000, y = 1.45, label="Expected\n value", color=muted("green"), size=5) +
            geom_curve(aes(x = 1000, xend=800, y=1.42, yend=1.25), arrow = arrow(length = unit(0.07, "inch")), size = 1,
            color = "gray20", curvature = -0.1) +
            scale_x_continuous(breaks = seq(0, n, by = 2000)) +
            scale_y_continuous(limits = c(1.15, 1.52)) + 
            labs(y = "Sample mean", title="Sample mean as trials increase (more trials!)", x="trials") + 
            theme(axis.title.x = element_text(margin = margin(t = 30, r = 0, b = 0, l = 0)))

graph2.animation <-p4 + transition_reveal(samples) + view_follow(fixed_y = TRUE)
animate(graph2.animation, height = 800, width = 800, fps = 10, duration = 5,
        end_pause = 10, res = 100)

```


We can clearly see that it took longer for the sample mean to reach the expected value in my distribution but it did reach it nonetheless! 

**Law of Large Numbers vindicated!**

### Why is this cool?

The Law of Large Numbers is a fundamental building block of statistics, as it allows us to substitute the expectation with an average. There are many scenarios where we know nothing about the expectation of a distribution we are interested in but we can use the law to estimate it.

